import httpx
import math
from typing import Dict, List, Any
from app.core.config import BASE_URL, API_KEY

def clean_company_symbol(symbol: str) -> str:
    """
    ØªÙ†Ø¸ÙŠÙ Ø±Ù…Ø² Ø§Ù„Ø´Ø±ÙƒØ© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
    """
    if not symbol:
        return ""
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù…Ø² ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ù‚Ø·Ø© (Ù…Ø«Ù„ 1050.SARE) Ù†Ø£Ø®Ø° Ø§Ù„Ø¬Ø²Ø¡ Ù‚Ø¨Ù„ Ø§Ù„Ù†Ù‚Ø·Ø© ÙÙ‚Ø·
    if '.' in symbol:
        clean_symbol = symbol.split('.')[0].upper().strip()
    else:
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù…Ø² Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ù‚Ø·Ø©ØŒ Ù†Ø³ØªØ®Ø¯Ù…Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ
        clean_symbol = symbol.upper().strip()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù‚ÙˆØ§Ø³ ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    clean_symbol = clean_symbol.replace('(', '').replace(')', '').strip()
    
    return clean_symbol

def is_valid_company(company: dict) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø´Ø±ÙƒØ© Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙ„ÙŠØ³Øª Ù…ÙƒØ±Ø±Ø© Ù…Ø²ÙŠÙØ©
    """
    symbol = company.get('symbol', '')
    name = company.get('name', '')
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ© Ù‡Ùˆ Ù†ÙØ³ Ø§Ù„Ø±Ù…Ø² (Ù…Ø«Ù„ "1050.SARE") ÙÙ‡ÙŠ Ù…Ø²ÙŠÙØ©
    if symbol and name and symbol.upper() == name.upper():
        return False
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³Ù… ÙØ§Ø±ØºØ§Ù‹ Ø£Ùˆ Ù„Ø§ Ù…Ø¹Ù†Ù‰ Ù„Ù‡
    if not name or name.strip() == '':
        return False
    
    # Ø´Ø±Ø· Ø¥Ø¶Ø§ÙÙŠ: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø§Ø³Ù… Ù„ÙŠØ³ Ù…Ø¬Ø±Ø¯ Ø£Ø±Ù‚Ø§Ù…
    if name.replace('.', '').replace(' ', '').isdigit():
        return False
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³Ù… Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ (Ø£Ù‚Ù„ Ù…Ù† 3 Ø£Ø­Ø±Ù)
    if len(name.strip()) < 3:
        return False
    
    return True

async def get_companies(page: int = 1, limit: int = 100, remove_duplicates: bool = True) -> Dict[str, Any]:
    """Fetch list of companies from the Saudi Stock Exchange (TADAWUL) with pagination and filtering."""
    try:
        print(f"ğŸ”„ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ§Øª Ù…Ù† API... Ø§Ù„ØµÙØ­Ø© {page}, Ø§Ù„Ø¹Ø¯Ø¯ {limit}, ØªØµÙÙŠØ©: {remove_duplicates}")
        url = f"{BASE_URL}/stocks"
        params = {
            "exchange": "TADAWUL",
            "apikey": API_KEY
        }

        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params)
            data = response.json()

        if "data" not in data:
            raise ValueError(f"Error fetching companies: {data}")

        all_companies = data["data"]
        print(f"âœ… ØªÙ… Ø¬Ù„Ø¨ {len(all_companies)} Ø´Ø±ÙƒØ© Ù…Ù† API")
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØµÙÙŠØ© Ø¥Ø°Ø§ Ù…Ø·Ù„ÙˆØ¨
        if remove_duplicates:
            cleaned_data = []
            seen_symbols = set()
            removed_count = 0
            
            for company in all_companies:
                # ØªØ®Ø·ÙŠ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ù…Ø²ÙŠÙØ©
                if not is_valid_company(company):
                    removed_count += 1
                    continue
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ù…Ø²
                original_symbol = company['symbol']
                clean_symbol = clean_company_symbol(original_symbol)
                
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù…Ø² ØºÙŠØ± Ù…ÙƒØ±Ø±ØŒ Ø£Ø¶ÙÙ‡
                if clean_symbol and clean_symbol not in seen_symbols:
                    seen_symbols.add(clean_symbol)
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ù…Ø² Ø¥Ù„Ù‰ Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù†Ø¸ÙŠÙØ©
                    company['symbol'] = clean_symbol
                    company['original_symbol'] = original_symbol
                    cleaned_data.append(company)
                else:
                    removed_count += 1
            
            all_companies = cleaned_data
            print(f"ğŸ¯ Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ©: {len(all_companies)} Ø´Ø±ÙƒØ© (ØªÙ… Ø­Ø°Ù {removed_count} Ø´Ø±ÙƒØ©)")
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù€ pagination Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ù…ØµÙØ§Ø© Ø£Ùˆ Ù„Ø§)
        start_index = (page - 1) * limit
        end_index = start_index + limit
        
        # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª within range
        total_companies = len(all_companies)
        if start_index >= total_companies:
            paginated_companies = []
            # Ø¥Ø°Ø§ Ø·Ù„Ø¨ ØµÙØ­Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø§Ø±Ø¬Ø¹ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
            if page > 1:
                start_index = 0
                end_index = limit
                paginated_companies = all_companies[start_index:end_index]
                page = 1
        else:
            paginated_companies = all_companies[start_index:end_index]
        
        # Ø­Ø³Ø§Ø¨ metadata Ù„Ù„Ù€ pagination
        total_pages = math.ceil(total_companies / limit) if total_companies > 0 else 1
        
        print(f"ğŸ“„ Pagination: Ø§Ù„ØµÙØ­Ø© {page}/{total_pages} - {len(paginated_companies)} Ø´Ø±ÙƒØ© Ù…Ù† Ø£ØµÙ„ {total_companies}")
        
        return {
            "data": paginated_companies,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total_companies,
                "total_pages": total_pages,
                "has_next": page < total_pages,
                "has_prev": page > 1,
                "next_page": page + 1 if page < total_pages else None,
                "prev_page": page - 1 if page > 1 else None
            }
        }
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ get_companies: {str(e)}")
        raise ValueError(f"Error in get_companies: {str(e)}")