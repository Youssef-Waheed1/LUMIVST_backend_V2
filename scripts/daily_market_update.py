import sys
from pathlib import Path
import csv
import traceback
import json
import logging
import datetime
from datetime import date, timedelta
from pathlib import Path
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.dialects.postgresql import insert
import sys

# Add parent directory to path
sys.path.append(str(Path(__file__).resolve().parent.parent))

from app.core.config import settings
from app.core.database import SessionLocal 
from app.models.price import Price
# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
from app.services.daily_detailed_scraper import scrape_daily_details
# âœ… Ø¨Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù€ Calculator Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù€ Calendar Ø¹Ø´Ø§Ù† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¯Ù‚Ø©
from scripts.rs_calculator_v3_calendar import calculate_daily_rs

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª
project_root = Path(__file__).resolve().parent.parent.parent

def load_industry_mapping():
    """
    ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹Ø§Øª
    """
    mapping = {}
    json_path = project_root / "backend" / "industry_mapping.json"
    
    # Try different paths just in case
    if not json_path.exists():
         json_path = project_root / "industry_mapping.json"
         
    try:
        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
            logger.info(f"Loaded {len(mapping)} industry mappings.")
        else:
            logger.warning("âš ï¸ Industry mapping file not found. New stocks might miss industry info.")
    except Exception as e:
        logger.error(f"âŒ Error loading industry mapping: {e}")
        
    return mapping

def update_daily():
    """
    1. Scrape Daily Data
    2. Save to DB (with correct Industry Group)
    3. Calculate RS (Incremental)
    """
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    db = SessionLocal()
    
    try:
        logger.info(f"ğŸš€ Starting Daily Market Update...")
        
        # 0. Load Mappings
        industry_map = load_industry_mapping()
        
        # 1. Scraping
        logger.info("ğŸ“¡ Scraping daily detailed report...")
        # Scraper returns list of dicts: {'Symbol': '...', 'Close': ...}
        scraped_data = scrape_daily_details(headless=True)
        
        if not scraped_data:
            logger.error("âŒ Scraping failed or returned no data.")
            return

        logger.info(f"ğŸ“Š Scraped {len(scraped_data)} records.")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ù†ÙØ³ Ø§Ù„Ù„ÙˆØ¬ÙŠÙƒ Ø¨ØªØ§Ø¹Ùƒ)
        now = datetime.datetime.now()
        market_date = date.today()
        # Ù„Ùˆ Ù‚Ø¨Ù„ Ø§Ù„Ø³Ø§Ø¹Ø© 3:30 (ÙˆÙ‚Øª Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø³ÙˆÙ‚ + Ù‚Ù„ÙŠÙ„)ØŒ Ù…Ù…ÙƒÙ† Ù†ÙƒÙˆÙ† Ø¨Ù†Ø­Ø¯Ø« Ø§Ù…Ø¨Ø§Ø±Ø­ØŸ
        # Ø¨Ø³ Ø§Ù„Ø£ØµØ­ Ù†Ø®Ù„ÙŠÙ‡Ø§ Ø¯Ø§ÙŠÙ…Ù‹Ø§ ØªØ§Ø±ÙŠØ® Ø§Ù„ÙŠÙˆÙ…ØŒ ÙˆÙ„Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø³Ø© Ù…Ù†Ø²Ù„ØªØ´ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ù‡ÙŠØ¬ÙŠØ¨ Ø§Ù„Ù‚Ø¯ÙŠÙ…ØŸ
        # Ù„Ø£ØŒ ØµÙØ­Ø© ØªØ¯Ø§ÙˆÙ„ Ø¨ØªØªØ­Ø¯Ø« ÙŠÙˆÙ…ÙŠÙ‹Ø§. Ù‡Ù†ÙØªØ±Ø¶ Ø¥Ù†Ù†Ø§ Ø¨Ù†Ø´ØºÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø¨Ø¹Ø¯ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚.
        
        logger.info(f"ğŸ“… Setting market date to: {market_date}")

        # 2. Saving Prices
        success_count = 0
        for item in scraped_data:
            symbol = item.get("Symbol")
            company = item.get("Company")
            
            if not symbol: continue

            # Get Industry Group
            # 1. Try from Scraper (unlikely for daily page)
            # 2. Try from Mapping File (Static)
            # 3. Fallback to "Unknown"
            industry = item.get("Industry Group") or item.get("Sector")
            if not industry:
                industry = industry_map.get(str(symbol), "Unknown")

            try:
                price_data = {
                    "symbol": symbol,
                    "date": market_date,
                    "open": item.get("Open", 0.0),
                    "high": item.get("Highest", 0.0),
                    "low": item.get("Lowest", 0.0),
                    "close": item.get("Close", 0.0),
                    "change": item.get("Change", 0.0),
                    "change_percent": item.get("Change %", 0.0),
                    "volume_traded": int(item.get("Volume Traded", 0)),
                    "value_traded_sar": float(item.get("Value Traded", 0.0)),
                    "company_name": company,
                    "industry_group": industry  # âœ… Added Industry
                }
                
                # Upsert
                stmt = insert(Price).values(price_data)
                stmt = stmt.on_conflict_do_update(
                    index_elements=['symbol', 'date'],
                    set_={
                        "open": stmt.excluded.open,
                        "high": stmt.excluded.high,
                        "low": stmt.excluded.low,
                        "close": stmt.excluded.close,
                        "change": stmt.excluded.change,
                        "change_percent": stmt.excluded.change_percent,
                        "volume_traded": stmt.excluded.volume_traded,
                        "value_traded_sar": stmt.excluded.value_traded_sar,
                        "company_name": stmt.excluded.company_name,
                        "industry_group": stmt.excluded.industry_group
                    }
                )
                db.execute(stmt)
                success_count += 1
            except Exception as row_error:
                logger.warning(f"âš ï¸ Skipped row {symbol}: {row_error}")
                continue
            
        db.commit()
        logger.info(f"âœ… Successfully saved/updated {success_count} price records.")

        # 3. RS Calculation (Optimized V3)
        # -------------------------------------------------------------------
        logger.info("ğŸ§® Starting RS Calculation (Incremental)...")
        
        # Calculate RS just for the target date
        # Note: We pass the DB URL string, not the session
        calculate_daily_rs(str(settings.DATABASE_URL), target_date=market_date)
        
        logger.info("ğŸ‰ Daily Update Workflow Completed Successfully!")

    except Exception as e:
        logger.error(f"âŒ Critical Error in Daily Update: {e}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    update_daily()
